{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-price",
   "metadata": {},
   "source": [
    "## **Imports required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-public",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "import recordlinkage\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "from recordlinkage.preprocessing import clean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from shapash.explainer.smart_explainer import SmartExplainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indoor-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipe_file_in_directory(dir_name: str, path_to_unzipe_files: str):\n",
    "    \"\"\"\n",
    "    Unzipe file from dir_name to path_to_unzipe_files directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        dir_name (str) : Folder in which files to unzip are \n",
    "        path_to_unzipe_files (str) : Where file have to be unzip\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    zip_files = os.listdir(dir_name)\n",
    "    working_dir = os.path.abspath('.')\n",
    "    os.chdir(dir_name) # change directory from working dir to dir with files\n",
    "\n",
    "    for item in zip_files: # loop through items in dir\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(path_to_unzipe_files) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name) # delete zipped file  \n",
    "    os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "important-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['variable_id', 'variable_label', 'unit', 'min_value', 'max_value', 'alias']\n",
    "\n",
    "def concate_file_from_different_directory_to_dataframe(file_to_find:str, dir_name: str, path_to_unzipe_files: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe concatenating all files named file_to_find present in all folders \n",
    "    contained in folder called dir_name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        file_to_find (str) : File to find in dir_name\n",
    "        dir_name (str) : Folder in which search is made\n",
    "        path_to_unzipe_files (str) : where files have to be unziped\n",
    "    \n",
    "    Return:\n",
    "    --------\n",
    "        concat_files (pandas.DataFrame) : A DataFrame with all unziped files concatenated\n",
    "    \n",
    "    \"\"\"\n",
    "    unzipe_file_in_directory(dir_name, path_to_unzipe_files)\n",
    "    os.chdir(path_to_unzipe_files) # change directory from working dir to dir with files\n",
    "    concat_files = pd.DataFrame()\n",
    "\n",
    "    for dir_ in os.listdir('.'):\n",
    "        if os.path.isdir(dir_): \n",
    "            dir_name = os.path.abspath(dir_)\n",
    "            for folder in os.listdir(dir_name):\n",
    "                if file_to_find in folder:\n",
    "                    tmp_data = pd.read_csv(dir_name + '/' + folder, header=None, encoding=\"ISO-8859-1\")\n",
    "                    concat_files = pd.concat([concat_files, tmp_data], ignore_index=True)\n",
    "    concat_files.columns = columns\n",
    "    return concat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "allied-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by_nan(dataset: pd.DataFrame, char: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace by NaN all char values find in dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        dataset (pandas.DataFrame) : DataFrame in which modifications are made\n",
    "        char (str) : Character to replace by nan \n",
    "        \n",
    "    Return:\n",
    "    -----------\n",
    "        preprocessed_data (pandas.DataFrame) : DataFrame with char values replaced by NaN\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocessed_data = dataset.copy()\n",
    "    for col in preprocessed_data.columns:\n",
    "        preprocessed_data[col] = preprocessed_data[col].apply(lambda x: np.nan if x == char else x)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "employed-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_recordlinkage(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make the preprocessinf of dataset using recordlinkage preprocessing function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        dataset (pandas.DataFrame) : DataFrame in which modifications are made\n",
    "        \n",
    "    Return:\n",
    "    -----------\n",
    "        preprocessed_data (pandas.DataFrame) : DataFrame with char values replaced by NaN\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocessed_data = dataset.copy()\n",
    "    for column in preprocessed_data.columns:\n",
    "        if isinstance(preprocessed_data[column][0], str) :\n",
    "            preprocessed_data[column] = recordlinkage.preprocessing.clean(preprocessed_data[column],\n",
    "                                                                          replace_by_none='()', lowercase=True, \n",
    "                                                                          strip_accents=None, remove_brackets=True,\n",
    "                                                                          encoding='utf-8', decode_error='strict')\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-truck",
   "metadata": {},
   "source": [
    "## **Reading files in zip folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "colonial-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '../datas/usine_datas/2.MariaDB_PCB_Schema_DUMP_ZIP_Files_Fab=CROLFA_Eqt=' \\\n",
    "'END10_Day=2020-11-02_Hrs=19h17-20h22'\n",
    "extension = \".zip\"\n",
    "path_to_unzipe_files = '../datas/usine_datas_unziped/2.MariaDB_PCB_Schema_DUMP_ZIP_Files_Fab=CROLFA_Eqt=' \\\n",
    "'END10_Day=2020-11-02_Hrs=19h17-20h22'\n",
    "file_to_find = 'PCB.Variables.dump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "economic-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = concate_file_from_different_directory_to_dataframe(file_to_find, dir_name, path_to_unzipe_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-fountain",
   "metadata": {},
   "source": [
    "#### 1.**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "victorian-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = replace_by_nan(variables, '\\\\N')\n",
    "variables = preprocessing_recordlinkage(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wound-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_subset = variables[['variable_id', 'variable_label', 'alias']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-cattle",
   "metadata": {},
   "source": [
    "####  2. **INDEXING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "covered-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_indexing(blocking_key: list, dataset: pd.DataFrame) -> pd.MultiIndex :\n",
    "    \"\"\"\n",
    "    Simple function that use block indexation from recordlinkage package.\n",
    "    \n",
    "    Make candidate record pairs, from dataset, that agree on one or more variables of blocking_key parameter. \n",
    "    Returns all record pairs founded.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        blocking_key (list) : A list of variables in which block method is made\n",
    "        dataset (pandas.DataFrame) : A dataframe containing at least blocking_key variables \n",
    "    \n",
    "    Return:\n",
    "    --------\n",
    "        pairs : pandas.MultiIndex with record pairs founded\n",
    "    \"\"\"\n",
    "    \n",
    "    indexer = recordlinkage.Index()\n",
    "    for key in blocking_key:\n",
    "        indexer.block(on=key)\n",
    "    pairs = indexer.index(dataset)\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fourth-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = block_indexing(['alias'], variables_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-latex",
   "metadata": {},
   "source": [
    "#### 3. **COMPARISON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adverse-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_scores(attr: list, candidate_pairs: pd.MultiIndex, dataset: pd.DataFrame, method:list=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare the attributes of candidate record pairs candidate_pairs and return scores comparison \n",
    "    returned by .compute() method from recordlinkage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        attr (list) : List of attributes to compare\n",
    "        candidate_pairs (pandas.MultiIndex) :  MultiIndex with index of candidates pairs to compare \n",
    "        original_dataset (pandas.DataFrame) : A dataframe containing at least feature_to_comp  \n",
    "        duplicate_dataset (pandas.DataFrame) :  A dataframe containing at least feature_to_comp \n",
    "        method (list) : list of method (jarowinkler, leveinshtein, etc) to use for comparison\n",
    "        \n",
    "    Return:\n",
    "    --------\n",
    "        scores (pandas.DataFrame) : A DataFrame with the comparison scores vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    if method is None:\n",
    "        method = ['jarowinkler']*len(attr)\n",
    "    \n",
    "    compare = recordlinkage.Compare()\n",
    "    # initialise similarity measurement algorithms\n",
    "    for i in range(len(attr)): \n",
    "        compare.string(attr[i], attr[i], label=attr[i]+'_score', method=method[i])\n",
    "\n",
    "    # the method .compute() returns the DataFrame with the feature vectors.\n",
    "    scores = compare.compute(candidate_pairs, dataset)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sought-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = comparison_scores(['variable_label'], candidate_pairs, variables_subset, ['levenshtein'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-reputation",
   "metadata": {},
   "source": [
    "### **DROP DOUBLON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "corporate-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_doublon(scores: pd.DataFrame, dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Drop duplicated row in dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        scores (pd.DataFrame) : A DataFrame with scores vector\n",
    "        dataset (pd.DataFrame) : DataFrame to deduplicate\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "        without_doublon (pandas.DataFrame) : A DataFrame without duplicated row\n",
    "    \"\"\"\n",
    "    \n",
    "    duplicated = scores[scores[scores.columns[0]] >= 1]\n",
    "    not_duplicated = scores[scores[scores.columns[0]] < 1]\n",
    "    \n",
    "    idx_to_keep = list(set(list(map(list, zip(*duplicated.index)))[0]) -   \n",
    "                   set(list(map(list, zip(*duplicated.index)))[1])) + \\\n",
    "                list(set(itertools.chain(*not_duplicated.index.to_list())))\n",
    "    \n",
    "    without_doublon = dataset.iloc[dataset.index.isin(idx_to_keep)]\n",
    "    \n",
    "    return without_doublon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "electric-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_without_doublon = drop_doublon(scores, variables_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "roman-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_id</th>\n",
       "      <th>variable_label</th>\n",
       "      <th>alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>110</td>\n",
       "      <td>pvd current target life 06</td>\n",
       "      <td>counter target life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111</td>\n",
       "      <td>pvd current target life 07</td>\n",
       "      <td>counter target life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112</td>\n",
       "      <td>pvd current target life 08</td>\n",
       "      <td>counter target life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114</td>\n",
       "      <td>ch recipe time 00</td>\n",
       "      <td>time recipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>120</td>\n",
       "      <td>ch recipe time 06</td>\n",
       "      <td>time recipe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable_id              variable_label                alias\n",
       "5          110  pvd current target life 06  counter target life\n",
       "6          111  pvd current target life 07  counter target life\n",
       "7          112  pvd current target life 08  counter target life\n",
       "8          114           ch recipe time 00          time recipe\n",
       "9          120           ch recipe time 06          time recipe"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_without_doublon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-disco",
   "metadata": {},
   "source": [
    "#### 4. **CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "addressed-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['label'] = scores['variable_label_score'].apply(lambda x: 1 if x > 0.96 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proved-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(scores, test_size=0.25)\n",
    "X_train = train.drop('label', axis=1, inplace=False)\n",
    "X_test = test.drop('label', axis=1, inplace=False)\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "detailed-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_track(project_name: str, clf, X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train and fit the classifier clf with X_train, X_test and y_train and calculate carbon emissions \n",
    "    using EmissionsTracker from codecarbon\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        project_name (str) : Name of the project for EmissionTracker function\n",
    "        X_train (pandas.DataFrame) : A DataFrame for training\n",
    "        X_test (pandas.DataFrame) : A DataFrame for test\n",
    "        y_train (pandas.DataFrame) : A DataFrame for training\n",
    "    Return:\n",
    "    -----------\n",
    "        predictions (numpy.array) : Array of predicted values\n",
    "        \n",
    "    \"\"\"\n",
    "    tracker = EmissionsTracker(project_name=project_name)\n",
    "    tracker.start()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    tracker.stop()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "geological-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts\n",
      "INFO:apscheduler.scheduler:Added job \"BaseEmissionsTracker._measure_power\" to job store \"default\"\n",
      "INFO:apscheduler.scheduler:Scheduler started\n",
      "INFO:apscheduler.scheduler:Scheduler has been shut down\n"
     ]
    }
   ],
   "source": [
    "classifier = LGBMClassifier()\n",
    "predictions = fit_and_track(\"donnees_usine\", classifier, X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-rochester",
   "metadata": {},
   "source": [
    "#### 4.**HUMAN REVIEW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "applicable-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "     'variable_label': 'variable_label'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "middle-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(X_test: pd.DataFrame,  y_test: pd.DataFrame, classifier: object, features: dict):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        X_test (pd.DataFrame) : DataFrame used to train classifier\n",
    "        y_test (pd.DataFrme) : DataFrame used to train classifier\n",
    "        features (dict) : Features present in DataFrame\n",
    "        classifier (object) : classifier used to make prediction \n",
    "        \n",
    "    Return: \n",
    "    -----------\n",
    "        xpl (object) : SmartExplainer \n",
    "    \"\"\"\n",
    "    \n",
    "    xpl = SmartExplainer(features_dict=features) # optional parameter\n",
    "    xpl.compile(\n",
    "        x=X_test,\n",
    "        model=classifier,\n",
    "        y_pred= y_test\n",
    "    )\n",
    "    return xpl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "proof-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C extension was not built during install!\n",
      "Backend: Shap TreeExplainer\n"
     ]
    }
   ],
   "source": [
    "explainer = explain(X_test, y_test, classifier, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdf_innovation_venv",
   "language": "python",
   "name": "tdf_innovation_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
